"# ğŸ¦ SHAP-Guided Knowledge Distillation for Credit Scoring

**åŸºäºSHAPç‰¹å¾é‡è¦æ€§å¼•å¯¼çš„çŸ¥è¯†è’¸é¦ä¿¡ç”¨è¯„åˆ†ç³»ç»Ÿ**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3+-green.svg)](https://scikit-learn.org/)
[![SHAP](https://img.shields.io/badge/SHAP-0.42+-orange.svg)](https://shap.readthedocs.io/)
[![Optuna](https://img.shields.io/badge/Optuna-3.4+-purple.svg)](https://optuna.org/)

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªåˆ›æ–°çš„ä¿¡ç”¨è¯„åˆ†æ¨¡å‹ä¼˜åŒ–ç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆ**SHAPç‰¹å¾é‡è¦æ€§åˆ†æ**å’Œ**çŸ¥è¯†è’¸é¦æŠ€æœ¯**ï¼Œå°†å¤æ‚çš„ç¥ç»ç½‘ç»œæ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å¯è§£é‡Šçš„å†³ç­–æ ‘å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œåœ¨ä¿æŒé«˜é¢„æµ‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

### ğŸ¯ æ ¸å¿ƒåˆ›æ–°ç‚¹

- **ğŸ§  å¤šæ¶æ„ç¥ç»ç½‘ç»œ**: é’ˆå¯¹ä¸åŒæ•°æ®é›†è®¾è®¡ä¸“é—¨çš„ç¥ç»ç½‘ç»œæ¶æ„
- **ğŸ” SHAPç‰¹å¾é‡è¦æ€§**: åŸºäºSHAPå€¼è¿›è¡Œç‰¹å¾é€‰æ‹©å’Œé‡è¦æ€§æ’åº
- **ğŸ“ çŸ¥è¯†è’¸é¦**: ä»å¤æ‚ç¥ç»ç½‘ç»œå‘å¯è§£é‡Šå†³ç­–æ ‘ä¼ é€’çŸ¥è¯†
- **âš¡ æ™ºèƒ½ä¼˜åŒ–**: é›†æˆOptunaè¿›è¡Œè‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–
- **ğŸ“Š å…¨é¢è¯„ä¼°**: å¤šç»´åº¦æ€§èƒ½åˆ†æå’Œå¯è§†åŒ–

## ğŸ“ é¡¹ç›®ç»“æ„

```
ğŸ“¦ shapGuided_KnowledgeDistilling/
â”œâ”€â”€ ğŸ“Š data/                          # æ•°æ®é›†ç›®å½•
â”‚   â”œâ”€â”€ uci_credit.xls                # UCIä¿¡ç”¨æ•°æ®é›†
â”‚   â”œâ”€â”€ german_credit.csv             # Germanä¿¡ç”¨æ•°æ®é›†
â”‚   â””â”€â”€ australian_credit.csv         # Australianä¿¡ç”¨æ•°æ®é›†
â”œâ”€â”€ ğŸ§  models/                        # è®­ç»ƒå¥½çš„æ¨¡å‹å­˜å‚¨
â”œâ”€â”€ ğŸ“ˆ results/                       # å®éªŒç»“æœ
â”œâ”€â”€ ğŸ“Š visualization/                 # å¯è§†åŒ–å›¾è¡¨
â”œâ”€â”€ ğŸ”§ data_preprocessing.py          # æ•°æ®é¢„å¤„ç†æ¨¡å—
â”œâ”€â”€ ğŸ§  neural_models.py               # ç¥ç»ç½‘ç»œæ•™å¸ˆæ¨¡å‹
â”œâ”€â”€ ğŸ” shap_analysis.py               # SHAPç‰¹å¾é‡è¦æ€§åˆ†æ
â”œâ”€â”€ ğŸ“ distillation_module.py         # çŸ¥è¯†è’¸é¦æ ¸å¿ƒæ¨¡å—
â”œâ”€â”€ ğŸ“Š experiment_manager.py          # å®éªŒç®¡ç†å’Œç»“æœåˆ†æ
â”œâ”€â”€ ğŸŒ³ tree_rules_analyzer.py         # å†³ç­–æ ‘è§„åˆ™æå–
â”œâ”€â”€ ğŸš€ main.py                        # ä¸»ç¨‹åºå…¥å£
â””â”€â”€ ğŸ“– README.md                      # é¡¹ç›®æ–‡æ¡£
```

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### 1. æ•°æ®é¢„å¤„ç†å±‚
- **æ ‡å‡†åŒ–å¤„ç†**: Z-scoreæ ‡å‡†åŒ–ç¡®ä¿ç‰¹å¾å°ºåº¦ä¸€è‡´
- **ç¼–ç è½¬æ¢**: åˆ†ç±»å˜é‡è‡ªåŠ¨ç¼–ç å¤„ç†
- **æ•°æ®åˆ†å‰²**: è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†æ™ºèƒ½åˆ†å‰²

### 2. æ•™å¸ˆæ¨¡å‹å±‚
```python
# UCIæ•°æ®é›† - å¤šå±‚æ„ŸçŸ¥æœº
MLP_UCI: Input â†’ 64 â†’ 32 â†’ 1 â†’ Sigmoid

# Germanæ•°æ®é›† - å¾„å‘åŸºå‡½æ•°ç½‘ç»œ  
RBF_German: Input â†’ 30 RBF Centers â†’ Linear â†’ Sigmoid

# Australianæ•°æ®é›† - è‡ªç¼–ç å™¨å¢å¼ºMLP
AE_MLP_Australian: Input â†’ Encoder(16â†’8) â†’ Decoder â†’ Classifier â†’ Sigmoid
```

### 3. ç‰¹å¾é‡è¦æ€§åˆ†æ
- **SHAPå€¼è®¡ç®—**: åŸºäºShapleyå€¼çš„ç‰¹å¾è´¡çŒ®åˆ†æ
- **ç‰¹å¾æ’åº**: æŒ‰é‡è¦æ€§å¯¹ç‰¹å¾è¿›è¡Œæ’åº
- **Top-Ké€‰æ‹©**: è‡ªé€‚åº”é€‰æ‹©æœ€é‡è¦çš„Kä¸ªç‰¹å¾

### 4. çŸ¥è¯†è’¸é¦å±‚
- **æ¸©åº¦ç¼©æ”¾**: è½¯æ ‡ç­¾æ¦‚ç‡è°ƒèŠ‚ (T âˆˆ [1,5])
- **æŸå¤±å‡½æ•°**: è’¸é¦æŸå¤± + ç¡¬æ ‡ç­¾æŸå¤±
- **æƒé‡å¹³è¡¡**: Î± âˆˆ [0.1, 0.9] æ§åˆ¶æŸå¤±æƒé‡

### 5. å­¦ç”Ÿæ¨¡å‹ä¼˜åŒ–
- **å†³ç­–æ ‘**: å¯è§£é‡Šçš„æ ‘çŠ¶ç»“æ„
- **æ·±åº¦æ§åˆ¶**: max_depth âˆˆ [4,8]
- **Optunaä¼˜åŒ–**: è‡ªåŠ¨æœç´¢æœ€ä¼˜è¶…å‚æ•°

## âš™ï¸ æ ¸å¿ƒç®—æ³•

### çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°

```python
L_total = Î± Ã— L_distillation + (1-Î±) Ã— L_hard

L_distillation = KL_divergence(
    softmax(logits_student / T), 
    softmax(logits_teacher / T)
) Ã— TÂ²

L_hard = CrossEntropy(logits_student, true_labels)
```

### SHAPç‰¹å¾é€‰æ‹©ç®—æ³•

```python
def shap_feature_selection(model, X_train, top_k):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_train)
    importance = np.mean(np.abs(shap_values), axis=0)
    return np.argsort(importance)[-top_k:]
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
Python >= 3.8
torch >= 2.0.0
scikit-learn >= 1.3.0
shap >= 0.42.0
optuna >= 3.4.0
pandas >= 1.5.0
numpy >= 1.24.0
matplotlib >= 3.6.0
seaborn >= 0.12.0
tqdm >= 4.64.0
```

### å®‰è£…ä¾èµ–

```bash
pip install torch scikit-learn shap optuna pandas numpy matplotlib seaborn tqdm openpyxl
```

### è¿è¡Œå®éªŒ

```bash
python main.py
```

## ğŸ“Š å®éªŒé…ç½®

### è¶…å‚æ•°è®¾ç½®

| å‚æ•° | èŒƒå›´ | è¯´æ˜ |
|------|------|------|
| `top_k` | [5, 10, 15, 20] | SHAPç‰¹å¾é€‰æ‹©æ•°é‡ |
| `temperature` | [1, 2, 3, 4, 5] | çŸ¥è¯†è’¸é¦æ¸©åº¦å‚æ•° |
| `alpha` | [0.1, 0.2, ..., 0.9] | æŸå¤±æƒé‡å¹³è¡¡å‚æ•° |
| `max_depth` | [4, 5, 6, 7, 8] | å†³ç­–æ ‘æœ€å¤§æ·±åº¦ |

### Optunaä¼˜åŒ–å‚æ•°

- `min_samples_split`: [2, 20]
- `min_samples_leaf`: [1, 10] 
- `max_features`: ['sqrt', 'log2', None]
- `criterion`: ['gini', 'entropy']

## ğŸ“ˆ æ€§èƒ½è¯„ä¼°

### è¯„ä¼°æŒ‡æ ‡

- **å‡†ç¡®ç‡ (Accuracy)**: ä¸»è¦è¯„åˆ¤æ ‡å‡†
- **F1åˆ†æ•°**: å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡
- **ç²¾ç¡®ç‡ (Precision)**: æ­£ä¾‹é¢„æµ‹å‡†ç¡®æ€§
- **å¬å›ç‡ (Recall)**: æ­£ä¾‹è¦†ç›–å®Œæ•´æ€§

### è¾“å‡ºæ–‡ä»¶

| æ–‡ä»¶ç±»å‹ | æè¿° |
|----------|------|
| `ğŸ“Š comparison_results.xlsx` | å››ç§æ¨¡å‹æ€§èƒ½å¯¹æ¯” |
| `ğŸ“‹ master_results.xlsx` | å®Œæ•´å®éªŒç»“æœè®°å½• |
| `ğŸ“ˆ performance_analysis.png` | æ€§èƒ½åˆ†æå›¾è¡¨ |
| `ğŸ” topk_parameter_analysis.png` | Top-Kå‚æ•°å½±å“åˆ†æ |
| `ğŸŒ³ decision_tree_rules.xlsx` | æœ€ä¼˜å†³ç­–æ ‘è§„åˆ™ |
| `ğŸ“„ experiment_summary.txt` | å®éªŒæ€»ç»“æŠ¥å‘Š |

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### æ•°æ®é›†ç‰¹ç‚¹

| æ•°æ®é›† | æ ·æœ¬æ•° | ç‰¹å¾æ•° | ä¸å¹³è¡¡æ¯”ä¾‹ | ç¥ç»ç½‘ç»œæ¶æ„ |
|--------|--------|--------|------------|--------------|
| UCI Credit | 30,000 | 23 | 22:78 | MLP (64â†’32â†’1) |
| German Credit | 1,000 | 20 | 70:30 | RBF (30 centers) |
| Australian Credit | 690 | 14 | 44:56 | AutoEncoder-MLP |

### ç®—æ³•å¤æ‚åº¦

- **SHAPåˆ†æ**: O(n Ã— m Ã— d) - næ ·æœ¬, mç‰¹å¾, dæ·±åº¦
- **çŸ¥è¯†è’¸é¦**: O(epochs Ã— batch_size Ã— parameters)
- **å†³ç­–æ ‘è®­ç»ƒ**: O(n Ã— log(n) Ã— m)

## ğŸ“Š å®éªŒç»“æœç¤ºä¾‹

```
ğŸ† æœ€ä½³æ¨¡å‹æ€§èƒ½ (UCIæ•°æ®é›†):
   æ¨¡å‹ç±»å‹: Top-KçŸ¥è¯†è’¸é¦
   å‡†ç¡®ç‡: 0.8234
   F1åˆ†æ•°: 0.7891
   ç‰¹å¾æ•°: 15 (åŸå§‹23ä¸ª)
   å†³ç­–æ ‘æ·±åº¦: 6
   æ¸©åº¦å‚æ•°: 3
   æƒé‡å‚æ•°: 0.7
```

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹ç¥ç»ç½‘ç»œæ¶æ„

```python
# åœ¨ neural_models.py ä¸­ä¿®æ”¹
class Custom_Model(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),  # è‡ªå®šä¹‰å±‚æ•°
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(), 
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
```

### è°ƒæ•´è’¸é¦å‚æ•°

```python
# åœ¨ main.py ä¸­ä¿®æ”¹å®éªŒé…ç½®
config = {
    'top_k_values': [8, 12, 16, 24],      # è‡ªå®šä¹‰Kå€¼
    'temperature_values': [2, 4, 6],       # è‡ªå®šä¹‰æ¸©åº¦
    'alpha_values': [0.3, 0.5, 0.7],      # è‡ªå®šä¹‰æƒé‡
    'max_depth_values': [5, 7, 9]         # è‡ªå®šä¹‰æ·±åº¦
}
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº MIT è®¸å¯è¯å¼€æº - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ“ è”ç³»æ–¹å¼

- **é¡¹ç›®ä½œè€…**: [Your Name]
- **é‚®ç®±**: [your.email@example.com]
- **GitHub**: [https://github.com/yourusername/shapGuided_KnowledgeDistilling](https://github.com/yourusername/shapGuided_KnowledgeDistilling)

## ğŸ™ è‡´è°¢

- [SHAP](https://github.com/slundberg/shap) - ç”¨äºæ¨¡å‹è§£é‡Šæ€§åˆ†æ
- [Optuna](https://github.com/optuna/optuna) - ç”¨äºè¶…å‚æ•°ä¼˜åŒ–
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶
- [Scikit-learn](https://scikit-learn.org/) - æœºå™¨å­¦ä¹ å·¥å…·åŒ…

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. *Advances in neural information processing systems*, 30.

2. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.

3. Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: A next-generation hyperparameter optimization framework. *Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining*.

---

â­ **å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼**" 
