"""
ç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰æ¨¡å—
Neural Network Models Module
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
torch.manual_seed(RANDOM_SEED)

class MLP_UCI(nn.Module):
    """UCI Credit Dataset - MLPå¤šå±‚æ„ŸçŸ¥æœº"""
    def __init__(self, input_dim):
        super(MLP_UCI, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # è¾“å‡ºraw logitsï¼Œä¸ç”¨Sigmoid
        )

    def forward(self, x):
        return self.net(x)
    
    def predict_proba(self, x):
        """ç”¨äºæ¨ç†çš„æ¦‚ç‡è¾“å‡º"""
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥æ˜¯Tensor
            if isinstance(x, np.ndarray):
                x = torch.FloatTensor(x)
            elif not isinstance(x, torch.Tensor):
                x = torch.FloatTensor(x)
            
            logits = self.forward(x)
            probs = torch.sigmoid(logits)
            
            # å¯¹äºäºŒåˆ†ç±»ï¼Œè¿”å›ä¸¤ä¸ªç±»åˆ«çš„æ¦‚ç‡
            prob_positive = probs.numpy().flatten()
            prob_negative = 1 - prob_positive
            
            # è¿”å›å½¢çŠ¶ä¸º (n_samples, 2) çš„æ¦‚ç‡æ•°ç»„
            return np.column_stack([prob_negative, prob_positive])

class RBF_German(nn.Module):
    """German Credit Dataset - RBFå¾„å‘åŸºå‡½æ•°ç½‘ç»œ"""
    def __init__(self, input_dim, num_rbf=50):
        super(RBF_German, self).__init__()
        self.num_rbf = num_rbf
        self.centers = nn.Parameter(torch.randn(num_rbf, input_dim))
        self.beta = nn.Parameter(torch.ones(1)*1.0)
        self.linear = nn.Linear(num_rbf, 1)  # è¾“å‡ºraw logits

    def rbf_layer(self, x):
        x_expanded = x.unsqueeze(1)
        c_expanded = self.centers.unsqueeze(0)
        dist_sq = ((x_expanded - c_expanded)**2).sum(-1)
        return torch.exp(-self.beta * dist_sq)

    def forward(self, x):
        phi = self.rbf_layer(x)
        out = self.linear(phi)
        return out  # raw logits
    
    def predict_proba(self, x):
        """ç”¨äºæ¨ç†çš„æ¦‚ç‡è¾“å‡º"""
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥æ˜¯Tensor
            if isinstance(x, np.ndarray):
                x = torch.FloatTensor(x)
            elif not isinstance(x, torch.Tensor):
                x = torch.FloatTensor(x)
            
            logits = self.forward(x)
            probs = torch.sigmoid(logits)
            
            # å¯¹äºäºŒåˆ†ç±»ï¼Œè¿”å›ä¸¤ä¸ªç±»åˆ«çš„æ¦‚ç‡
            prob_positive = probs.numpy().flatten()
            prob_negative = 1 - prob_positive
            
            # è¿”å›å½¢çŠ¶ä¸º (n_samples, 2) çš„æ¦‚ç‡æ•°ç»„
            return np.column_stack([prob_negative, prob_positive])

class Autoencoder(nn.Module):
    """è‡ªç¼–ç å™¨"""
    def __init__(self, input_dim, latent_dim):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 16),
            nn.ReLU(),
            nn.Linear(16, latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 16),
            nn.ReLU(),
            nn.Linear(16, input_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon, z

class AE_MLP_Australian(nn.Module):
    """Australian Credit - è‡ªç¼–ç å™¨å¢å¼ºMLP"""
    def __init__(self, input_dim, latent_dim=8):
        super(AE_MLP_Australian, self).__init__()
        self.ae = Autoencoder(input_dim, latent_dim)
        self.classifier = nn.Sequential(
            nn.Linear(latent_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # è¾“å‡ºraw logits
        )

    def forward(self, x):
        _, z = self.ae(x)
        out = self.classifier(z)
        return out  # raw logits
    
    def predict_proba(self, x):
        """ç”¨äºæ¨ç†çš„æ¦‚ç‡è¾“å‡º"""
        with torch.no_grad():
            # ç¡®ä¿è¾“å…¥æ˜¯Tensor
            if isinstance(x, np.ndarray):
                x = torch.FloatTensor(x)
            elif not isinstance(x, torch.Tensor):
                x = torch.FloatTensor(x)
            
            logits = self.forward(x)
            probs = torch.sigmoid(logits)
            
            # å¯¹äºäºŒåˆ†ç±»ï¼Œè¿”å›ä¸¤ä¸ªç±»åˆ«çš„æ¦‚ç‡
            prob_positive = probs.numpy().flatten()
            prob_negative = 1 - prob_positive
            
            # è¿”å›å½¢çŠ¶ä¸º (n_samples, 2) çš„æ¦‚ç‡æ•°ç»„
            return np.column_stack([prob_negative, prob_positive])

class TeacherModelTrainer:
    """æ•™å¸ˆæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, device='cpu'):
        self.device = device
        
    def create_model(self, dataset_name, input_dim):
        """æ ¹æ®æ•°æ®é›†åˆ›å»ºå¯¹åº”çš„æ¨¡å‹"""
        if dataset_name == 'uci':
            return MLP_UCI(input_dim)
        elif dataset_name == 'german':
            return RBF_German(input_dim)
        elif dataset_name == 'australian':
            return AE_MLP_Australian(input_dim)
        else:
            raise ValueError(f"Unknown dataset: {dataset_name}")
    
    def train_model(self, model, X_train, y_train, X_val, y_val, 
                   epochs=200, lr=1e-3, weight_decay=1e-4):
        """è®­ç»ƒæ•™å¸ˆæ¨¡å‹"""
        model = model.to(self.device)
        
        # è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®
        pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])
        pos_weight_tensor = torch.FloatTensor([pos_weight]).to(self.device)
        
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)
        
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train).view(-1, 1).to(self.device)
        X_val_tensor = torch.FloatTensor(X_val).to(self.device)
        y_val_tensor = torch.FloatTensor(y_val).view(-1, 1).to(self.device)
        
        best_val_loss = float('inf')
        best_state = None
        patience_counter = 0
        patience = 15
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            optimizer.zero_grad()
            logits = model(X_train_tensor)
            loss = criterion(logits, y_train_tensor)
            loss.backward()
            optimizer.step()
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            with torch.no_grad():
                val_logits = model(X_val_tensor)
                val_loss = criterion(val_logits, y_val_tensor).item()
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_state = model.state_dict().copy()
                patience_counter = 0
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                break
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        model.load_state_dict(best_state)
        return model
    
    def train_teacher_model(self, dataset_name, data_dict):
        """è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼ˆä½¿ç”¨å›ºå®šæ¶æ„ï¼‰"""
        print(f"ğŸ¯ Training {dataset_name.upper()} teacher model...")
        
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model(dataset_name, data_dict['X_train'].shape[1])
        input_features = data_dict['X_train'].shape[1]
        
        # è·å–æ¨¡å‹æ¶æ„åç§°å’Œå‚æ•°æ•°é‡
        architecture_name = self.get_architecture_name(dataset_name)
        model_parameters = sum(p.numel() for p in model.parameters())
        
        import time
        start_time = time.time()
        
        # è®­ç»ƒæ¨¡å‹
        model = self.train_model(
            model, data_dict['X_train'], data_dict['y_train'],
            data_dict['X_val'], data_dict['y_val'],
            lr=1e-3, weight_decay=1e-4
        )
        
        training_time = time.time() - start_time
        
        # è¯„ä¼°æ€§èƒ½ - ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è€Œä¸æ˜¯å›ºå®šçš„0.5
        model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(data_dict['X_test']).to(self.device)
            logits = model(X_test_tensor)
            probs = torch.sigmoid(logits).cpu().numpy().flatten()
            
            # åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
            X_val_tensor = torch.FloatTensor(data_dict['X_val']).to(self.device)
            val_logits = model(X_val_tensor)
            val_probs = torch.sigmoid(val_logits).cpu().numpy().flatten()
            
            # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼ï¼ˆæœ€å¤§åŒ–F1åˆ†æ•°ï¼‰
            best_threshold = 0.5
            best_f1 = 0
            for threshold in np.arange(0.1, 0.9, 0.05):
                val_preds = (val_probs > threshold).astype(int)
                f1 = f1_score(data_dict['y_val'], val_preds, zero_division=0)
                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = threshold
            
            # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡Œæœ€ç»ˆé¢„æµ‹
            preds = (probs > best_threshold).astype(int)
            
            accuracy = accuracy_score(data_dict['y_test'], preds)
            f1 = f1_score(data_dict['y_test'], preds, zero_division=0)
            precision = precision_score(data_dict['y_test'], preds, zero_division=0)
            recall = recall_score(data_dict['y_test'], preds, zero_division=0)
        
        print(f"  Test Accuracy: {accuracy:.4f}, Test F1-Score: {f1:.4f}")
        
        return {
            'model': model,
            'model_type': architecture_name,
            'feature_names': [f'feature_{i}' for i in range(input_features)],
            'test_metrics': {
                'accuracy': accuracy,
                'f1_score': f1,
                'precision': precision,
                'recall': recall
            },
            'training_info': {
                'final_loss': 'N/A',  # BCEWithLogitsLossæœ€ç»ˆå€¼
                'epochs': 100,  # é»˜è®¤epochæ•°
                'parameters': model_parameters,
                'training_time': f"{training_time:.2f}s"
            }
        }
    
    def get_architecture_name(self, dataset_name):
        """æ ¹æ®æ•°æ®é›†åç§°è¿”å›æ¶æ„åç§°"""
        if dataset_name == 'uci':
            return 'MLP_UCI'
        elif dataset_name == 'german':
            return 'RBF_German'
        elif dataset_name == 'australian':
            return 'AE_MLP_Australian'
        else:
            return f'Neural_Network_{dataset_name.upper()}'
    
    def train_all_teacher_models(self, processed_data):
        """è®­ç»ƒæ‰€æœ‰æ•°æ®é›†çš„æ•™å¸ˆæ¨¡å‹"""
        print("ğŸ§  Training teacher models for all datasets...")
        
        teacher_models = {}
        
        for dataset_name, data_dict in processed_data.items():
            print(f"\nğŸ”¬ Training teacher model for {dataset_name} dataset...")
            teacher_models[dataset_name] = self.train_teacher_model(dataset_name, data_dict)
        
        print("\nâœ… All teacher models trained successfully!")
        return teacher_models
