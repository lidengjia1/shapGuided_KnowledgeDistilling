# SHAP-Guided Knowledge Distillation for Credit Scoring

## ğŸ¯ Project Overview

**åŸºäºSHAPç‰¹å¾é‡è¦æ€§å¼•å¯¼çš„çŸ¥è¯†è’¸é¦ä¿¡ç”¨è¯„åˆ†ç³»ç»Ÿ**

This project implements a comprehensive framework for **SHAP-guided knowledge distillation** in credit scoring applications. The system combines the interpretability of decision trees with the predictive power of deep neural networks through innovative knowledge distillation techniques, using SHAP (SHapley Additive exPlanations) for intelligent feature selection.

---

## ğŸ“ Project Structure

```
Financial innovation/
â”œâ”€â”€ data/                          # Dataset storage
â”‚   â”œâ”€â”€ german_credit.csv         # German Credit Dataset (1,000 samples, 54 features)
â”‚   â”œâ”€â”€ australian_credit.csv     # Australian Credit Dataset (690 samples, 22 features)
â”‚   â””â”€â”€ uci_credit.xls           # UCI Taiwan Credit Dataset (30,000 samples, 23 features)
â”œâ”€â”€ results/                       # Generated output files
â”‚   â”œâ”€â”€ model_comparison_*.xlsx    # Model performance comparison
â”‚   â”œâ”€â”€ shap_feature_importance.png # SHAP feature visualization
â”‚   â”œâ”€â”€ ablation_study_analysis_*.png # Ablation study plots
â”‚   â”œâ”€â”€ topk_ablation_study_analysis_*.png # Top-k ablation analysis
â”‚   â”œâ”€â”€ best_all_feature_rules_*.txt # Full feature decision rules
â”‚   â””â”€â”€ best_topk_rules_*.txt     # Top-k feature decision rules
â”œâ”€â”€ main.py                       # Main execution pipeline
â”œâ”€â”€ data_preprocessing.py         # Data loading and preprocessing
â”œâ”€â”€ neural_models.py             # Neural network teacher models
â”œâ”€â”€ distillation_module.py       # Knowledge distillation core
â”œâ”€â”€ shap_analysis.py             # SHAP feature importance analysis
â”œâ”€â”€ ablation_analyzer.py         # Ablation study visualization
â”œâ”€â”€ result_manager.py            # Output management and reporting
â””â”€â”€ README.md                    # This documentation
```

---

## ğŸ§  Teacher Model Architectures

### German Credit Dataset (1,000 samples, 54 features)
**Enhanced Residual Neural Network** - ä¼˜åŒ–çš„æ®‹å·®ç½‘ç»œæ¶æ„
- **Architecture**: Residual blocks with skip connections for improved gradient flow
- **Layers**:
  - Input: Linear(54 â†’ 512) + BatchNorm + ReLU + Dropout(0.3)
  - Residual Block 1: [Linear(512 â†’ 256) â†’ BatchNorm â†’ ReLU â†’ Linear(256 â†’ 256) â†’ BatchNorm] + Skip(512 â†’ 256)
  - Residual Block 2: [Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Linear(128 â†’ 128) â†’ BatchNorm] + Skip(256 â†’ 128)
  - Output: Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Linear(64 â†’ 32) â†’ ReLU â†’ Linear(32 â†’ 1)
- **Loss Function**: BCEWithLogitsLoss with class balancing (pos_weight for imbalanced data)
- **Optimization**: AdamW (lr=0.0005, weight_decay=1e-3), ReduceLROnPlateau scheduler
- **Training**: 100 epochs (optimized), patience=30, batch_size=32
- **Target Accuracy**: 75%+ (improved from previous 62%)
- **Reference**: Residual Networks (ResNet) - He et al. (2016)

### Australian Credit Dataset (690 samples, 22 features)  
**Deep Feed-Forward Network** - æ·±åº¦å‰é¦ˆç½‘ç»œ
- **Architecture**: Sequential layers with batch normalization and dropout
- **Layers**: 
  - Linear(22 â†’ 256) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.4)
  - Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.35)
  - Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.3)
  - Linear(64 â†’ 32) â†’ ReLU â†’ Dropout(0.25)
  - Linear(32 â†’ 1) â†’ Sigmoid
- **Loss Function**: BCELoss (balanced dataset)
- **Optimization**: AdamW (lr=0.002, weight_decay=1e-3), ReduceLROnPlateau scheduler  
- **Training**: 100 epochs (optimized), patience=20, batch_size=64
- **Expected Accuracy**: 85%+
- **Reference**: Deep Neural Networks for Credit Scoring - Khandani et al. (2010)

### UCI Credit Default Dataset (30,000 samples, 23 features)
**Large-Scale Deep Network** - å¤§è§„æ¨¡æ·±åº¦ç½‘ç»œ
- **Architecture**: Deep network optimized for large datasets
- **Layers**:
  - Linear(23 â†’ 512) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.5)
  - Linear(512 â†’ 256) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.45)
  - Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.4)
  - Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.35)
  - Linear(64 â†’ 32) â†’ ReLU â†’ Dropout(0.3)
  - Linear(32 â†’ 1) â†’ Sigmoid
- **Loss Function**: BCELoss with focal loss characteristics for large-scale training
- **Optimization**: AdamW (lr=0.001, weight_decay=1e-4), ReduceLROnPlateau scheduler
- **Training**: 100 epochs (optimized), patience=25, batch_size=128  
- **Expected Accuracy**: 82%+
- **Reference**: Large-scale Credit Scoring - Lessmann et al. (2015)

---

## ğŸ“Š Four-Model Comparison Framework

æœ¬ç³»ç»Ÿè®­ç»ƒå¹¶å¯¹æ¯”ä»¥ä¸‹å››ç§æ¨¡å‹ï¼š

### 1. Teacher Model (æ•™å¸ˆæ¨¡å‹)
- **æ¶æ„**: æ•°æ®é›†ç‰¹å®šçš„PyTorchæ·±åº¦ç¥ç»ç½‘ç»œ
- **ç‰¹ç‚¹**: é«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¤æ‚åº¦é«˜
- **ç›®çš„**: ä½œä¸ºçŸ¥è¯†è’¸é¦çš„æºæ¨¡å‹

### 2. Baseline Decision Tree (åŸºå‡†å†³ç­–æ ‘)
- **æ¶æ„**: æ ‡å‡†scikit-learn DecisionTreeClassifier
- **ç‰¹ç‚¹**: é«˜å¯è§£é‡Šæ€§ï¼Œç®€å•ç»“æ„
- **ç›®çš„**: æä¾›åŸºå‡†æ€§èƒ½å¯¹æ¯”

### 3. All-Feature Distillation (å…¨ç‰¹å¾è’¸é¦)
- **æ¶æ„**: ä½¿ç”¨å…¨éƒ¨ç‰¹å¾çš„çŸ¥è¯†è’¸é¦å†³ç­–æ ‘
- **ç‰¹ç‚¹**: å¹³è¡¡å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§
- **ç›®çš„**: å®Œæ•´ç‰¹å¾ç©ºé—´ä¸‹çš„çŸ¥è¯†è¿ç§»

### 4. Top-k Feature Distillation (Top-kç‰¹å¾è’¸é¦)
- **æ¶æ„**: åŸºäºSHAP Top-kç‰¹å¾çš„çŸ¥è¯†è’¸é¦å†³ç­–æ ‘
- **ç‰¹ç‚¹**: ç²¾ç®€ç‰¹å¾é›†ï¼Œé«˜æ•ˆè§£é‡Š
- **ç›®çš„**: æœ€ä¼˜ç‰¹å¾å­é›†ä¸‹çš„çŸ¥è¯†è¿ç§»

---

## ğŸ”¬ Knowledge Distillation Process

### æ ¸å¿ƒæŠ€æœ¯å‚æ•°
- **Temperature Scaling**: T âˆˆ {1, 2, 3, 4, 5} for soft label generation
- **Loss Combination**: Î± âˆˆ {0.0, 0.1, ..., 1.0} for balancing hard and soft losses
- **Dynamic Feature Selection**: 
  - German Dataset: k âˆˆ {5, 6, 7, ..., 54}
  - Australian Dataset: k âˆˆ {5, 6, 7, ..., 22}
  - UCI Dataset: k âˆˆ {5, 6, 7, ..., 23}
- **Tree Optimization**: Optuna-based hyperparameter tuning for decision trees
- **Decision Tree Depth**: max_depth âˆˆ {3, 4, 5, ..., 10}

### è’¸é¦è¿‡ç¨‹
1. **Teacher Training**: è®­ç»ƒæ•°æ®é›†ç‰¹å®šçš„æ·±åº¦ç¥ç»ç½‘ç»œ
2. **SHAP Analysis**: è®¡ç®—ç‰¹å¾é‡è¦æ€§å¹¶æ’åº
3. **Knowledge Transfer**: é€šè¿‡æ¸©åº¦ç¼©æ”¾è½¯æ ‡ç­¾è¿›è¡ŒçŸ¥è¯†è¿ç§»
4. **Student Optimization**: åŸºäºæ··åˆæŸå¤±å‡½æ•°ä¼˜åŒ–å†³ç­–æ ‘å­¦ç”Ÿæ¨¡å‹
5. **Rule Extraction**: ä»è®­ç»ƒå¥½çš„å†³ç­–æ ‘ä¸­æå–å¯è§£é‡Šè§„åˆ™

---
  - Linear(256 â†’ 128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.4)
  - Linear(128 â†’ 64) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.35)
  - Linear(64 â†’ 32) â†’ ReLU â†’ Dropout(0.3)
  - Linear(32 â†’ 1) â†’ Sigmoid
- **Loss Function**: BCELoss with focal loss characteristics for large-scale training
- **Optimization**: AdamW (lr=0.001, weight_decay=1e-4), ReduceLROnPlateau scheduler
- **Training**: 300 epochs, patience=25, batch_size=128  
- **Expected Accuracy**: 82%+

## ğŸ“Š Four-Model Comparison Framework

1. **Teacher Model**: Dataset-specific deep neural networks (architectures above)
2. **Baseline Decision Tree**: Standard scikit-learn DecisionTreeClassifier  
3. **All-Feature Distillation**: Knowledge distillation using complete feature set
4. **Top-k Feature Distillation**: SHAP-guided feature selection for targeted distillation

## ğŸ”¬ Knowledge Distillation Process

- **Temperature Scaling**: T âˆˆ {1, 2, 3, 4, 5} for soft label generation
- **Loss Combination**: Î± âˆˆ {0.0, 0.1, ..., 1.0} for balancing hard and soft losses
- **Feature Selection**: Dynamic k ranges (German: 5-54, Australian: 5-22, UCI: 5-23)
- **Tree Optimization**: Optuna-based hyperparameter tuning for decision trees

Financial innovation/

â”œâ”€â”€ data/                          # Dataset storage- **Knowledge Distillation**: å°†æ•™å¸ˆæ¨¡å‹çŸ¥è¯†è¿ç§»åˆ°å­¦ç”Ÿæ¨¡å‹

â”‚   â”œâ”€â”€ german_credit.csv          # German Credit Dataset

â”‚   â”œâ”€â”€ australian_credit.csv      # Australian Credit Dataset### ğŸ¯ Advanced Knowledge Distillation- **PyTorch Neural Networks**: é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ•™å¸ˆæ¨¡å‹

â”‚   â””â”€â”€ uci_credit.xls            # UCI Taiwan Credit Dataset

â”œâ”€â”€ results/                       # Output files (generated)- **Temperature-scaled Soft Labels**: Configurable temperature parameter (T âˆˆ {1,2,3,4,5})- **Decision Tree**: å¯è§£é‡Šæ€§å¼ºçš„å­¦ç”Ÿæ¨¡å‹

â”‚   â”œâ”€â”€ model_comparison_*.xlsx    # Performance comparison table

â”‚   â”œâ”€â”€ shap_feature_importance.png # SHAP visualization- **Hybrid Loss Function**: Balanced combination of hard and soft label losses (Î± âˆˆ {0.0,0.1,...,1.0})

â”‚   â””â”€â”€ best_topk_rules_*.txt      # Extracted decision rules

â”œâ”€â”€ trained_models/               # Saved models (generated)- **Multi-depth Decision Trees**: Adaptive tree depth optimization (3-10 levels)---

â”‚   â”œâ”€â”€ teacher_model_*.pth       # PyTorch teacher models

â”‚   â”œâ”€â”€ teacher_model_*.pkl       # Scikit-learn format

â”‚   â””â”€â”€ teacher_model_*.json      # Model metadata

â”œâ”€â”€ main.py                       # Main execution pipeline### ğŸ“Š SHAP-Based Feature Selection  ## ç³»ç»Ÿæ¶æ„

â”œâ”€â”€ data_preprocessing.py         # Data loading and preprocessing

â”œâ”€â”€ neural_models.py             # Neural network architectures- **Intelligent Feature Ranking**: TreeExplainer-based SHAP value computation

â”œâ”€â”€ distillation_module.py       # Knowledge distillation implementation

â”œâ”€â”€ shap_analysis.py             # SHAP feature importance analysis- **Top-k Selection**: Systematic evaluation of k âˆˆ {5,6,7,8} most important features```

â”œâ”€â”€ result_manager.py            # Output management and reporting

â”œâ”€â”€ teacher_model_saver.py       # Model serialization utilities- **Cross-Dataset Analysis**: Comparative feature importance across datasetsâ”œâ”€â”€ data/                          # æ•°æ®é›†

â””â”€â”€ README.md                    # This documentation

```â”‚   â”œâ”€â”€ uci_credit.xls            # UCIä¿¡ç”¨å¡æ•°æ®é›†



## ï¿½ SHAP Feature Analysis

### SHAPæ–¹æ³•ç‰¹ç‚¹
- **TreeExplainer**: é’ˆå¯¹å†³ç­–æ ‘æ¨¡å‹ä¼˜åŒ–çš„SHAPè§£é‡Šå™¨
- **å…¨æ•°æ®é›†åˆ†æ**: ä½¿ç”¨è®­ç»ƒ+éªŒè¯+æµ‹è¯•çš„å®Œæ•´æ•°æ®é›†
- **ç²¾ç¡®ç‰¹å¾æ’åº**: åŸºäºå¹³å‡ç»å¯¹SHAPå€¼è¿›è¡Œç‰¹å¾é‡è¦æ€§æ’å
- **å¯è§†åŒ–è¾“å‡º**: ç”ŸæˆTop-20ç‰¹å¾çš„å¯¹æ¯”å›¾è¡¨

### ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
- **æ•°æ®é›†é¡ºåº**: German â†’ Australian â†’ UCI
- **é¢œè‰²æ–¹æ¡ˆ**: æµ…è“è‰²ç³» â†’ æµ…ç»¿è‰²ç³» â†’ æµ…æ©™è‰²ç³»
- **ç‰¹å¾æ•°é‡**: æ¯ä¸ªæ•°æ®é›†æ˜¾ç¤ºTop-20é‡è¦ç‰¹å¾
- **çœŸå®ç‰¹å¾å**: ä½¿ç”¨è‹±æ–‡åŸå§‹ç‰¹å¾åè€Œéç¼–ç å

---

## ğŸ”§ Core Modules

### 1. Data Preprocessing (`data_preprocessing.py`)
- **åŠŸèƒ½**: åŠ è½½å’Œé¢„å¤„ç†ä¸‰ä¸ªä¿¡ç”¨æ•°æ®é›†
- **æ ¸å¿ƒç‰¹æ€§**:
  - æ ‡å‡†åŒ–çš„æ•°æ®åŠ è½½å’Œtrain/validation/teståˆ’åˆ†
  - åˆ†ç±»å˜é‡çš„ç‰¹å¾ç¼–ç 
  - æ•°æ®ç¼©æ”¾å’Œæ ‡å‡†åŒ–
  - ç‰¹å¾åè¿½è¸ªä»¥ä¿è¯å¯è§£é‡Šæ€§

### 2. Neural Network Models (`neural_models.py`)
- **åŠŸèƒ½**: å®šä¹‰å’Œè®­ç»ƒæ•™å¸ˆç¥ç»ç½‘ç»œ
- **æ¶æ„ç‰¹ç‚¹**:
  - å¸¦æ®‹å·®è¿æ¥çš„é«˜çº§å‰é¦ˆç½‘ç»œ
  - æ‰¹é‡æ ‡å‡†åŒ–å’Œdropoutæ­£åˆ™åŒ–
  - è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦
  - æ—©åœå’Œæ¨¡å‹æ£€æŸ¥ç‚¹

### 3. SHAP Analysis (`shap_analysis.py`)
- **åŠŸèƒ½**: ä½¿ç”¨SHAPè¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ
- **å¤„ç†æµç¨‹**:
  - ä¸ºæ¯ä¸ªæ•°æ®é›†è®­ç»ƒä¼˜åŒ–çš„å†³ç­–æ ‘
  - ä½¿ç”¨TreeExplainerè®¡ç®—SHAPå€¼
  - ç”Ÿæˆtop-kç‰¹å¾æ’å
  - åˆ›å»ºå¸¦æœ‰æ­£ç¡®ç‰¹å¾åçš„å¯è§†åŒ–

### 4. Knowledge Distillation (`distillation_module.py`)
- **åŠŸèƒ½**: ä»æ•™å¸ˆæ¨¡å‹å‘å­¦ç”Ÿæ¨¡å‹è½¬ç§»çŸ¥è¯†
- **å®ç°ç»†èŠ‚**:
  - æ¸©åº¦ç¼©æ”¾çš„è½¯æ ‡ç­¾ç”Ÿæˆ
  - æ··åˆæŸå¤±å‡½æ•°(ç¡¬æ ‡ç­¾+è½¯æ ‡ç­¾)
  - åŸºäºSHAPçš„top-kç‰¹å¾é€‰æ‹©
  - ä»è®­ç»ƒå¥½çš„æ ‘ä¸­æå–å†³ç­–è§„åˆ™

### 5. Result Management (`result_manager.py`)
- **åŠŸèƒ½**: ç»„ç»‡å’Œå¯¼å‡ºç»“æœ
- **è¾“å‡ºå†…å®¹**:
  - åŸºäºExcelçš„æ€§èƒ½å¯¹æ¯”
  - å†³ç­–è§„åˆ™æ–‡æœ¬æ–‡ä»¶
  - æ¨¡å‹æ€§èƒ½æŒ‡æ ‡

### 6. Ablation Analysis (`ablation_analyzer.py`)
- **åŠŸèƒ½**: æ¶ˆèå®éªŒåˆ†æå’Œå¯è§†åŒ–
- **è¾“å‡ºå›¾è¡¨**:
  - Top-kç‰¹å¾æ•°é‡æ¶ˆèå®éªŒ
  - å†³ç­–æ ‘æ·±åº¦æ¶ˆèå®éªŒ
  - 1Ã—2å¸ƒå±€çš„ç®€åŒ–å›¾è¡¨

---

## ğŸ“ˆ Datasets

ç³»ç»Ÿåœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä¿¡ç”¨è¯„åˆ†åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼š

### 1. German Credit Dataset (1,000 samples, 54 features)
- **æ¥æº**: UCI Machine Learning Repository
- **ä»»åŠ¡**: äºŒåˆ†ç±»(å¥½/åä¿¡ç”¨é£é™©)
- **ç‰¹å¾**: äººå£ç»Ÿè®¡å­¦ã€è´¦æˆ·çŠ¶æ€ã€ä¿¡ç”¨å†å²

### 2. Australian Credit Approval Dataset (690 samples, 22 features)
- **æ¥æº**: UCI Machine Learning Repository  
- **ä»»åŠ¡**: äºŒåˆ†ç±»(æ‰¹å‡†/æ‹’ç»ä¿¡ç”¨)
- **ç‰¹å¾**: åŒ¿ååŒ–çš„ç”³è¯·äººå±æ€§

### 3. Taiwan Credit Card Default Dataset (30,000 samples, 23 features)
- **æ¥æº**: UCI Machine Learning Repository
- **ä»»åŠ¡**: äºŒåˆ†ç±»(è¿çº¦/éè¿çº¦)
- **ç‰¹å¾**: æ”¯ä»˜å†å²ã€è´¦å•é‡‘é¢ã€äººå£ç»Ÿè®¡æ•°æ®

---

## ğŸš€ Installation & Usage

### Prerequisites
```bash
pip install torch scikit-learn pandas numpy matplotlib seaborn shap openpyxl optuna tqdm
```

### Quick Start
```bash
# Clone the repository
git clone https://github.com/lidengjia1/shapGuided_KnowledgeDistilling.git
cd shapGuided_KnowledgeDistilling

# Run the complete pipeline
python main.py
```

### Expected Outputs
è¿è¡Œå®Œæˆåï¼Œå°†ç”Ÿæˆä»¥ä¸‹æ ¸å¿ƒæ–‡ä»¶ï¼š

1. **æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨** (`results/model_comparison_*.xlsx`)
   - å››ç§æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡
   - å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç²¾ç¡®ç‡ã€å¬å›ç‡
   - æ¯ç§é…ç½®çš„æœ€ä½³è¶…å‚æ•°

2. **SHAPç‰¹å¾é‡è¦æ€§å›¾** (`results/shap_feature_importance.png`)
   - ä¸‰ä¸ªæ•°æ®é›†çš„å¯è§†åŒ–å¯¹æ¯”
   - æ¯ä¸ªæ•°æ®é›†çš„Top-20é‡è¦ç‰¹å¾
   - è‹±æ–‡æ ‡ç­¾å’Œæ­£ç¡®çš„ç‰¹å¾å

3. **Top-kå†³ç­–è§„åˆ™** (`results/best_topk_rules_*.txt`)
   - ä»æœ€ä½³æ¨¡å‹æå–çš„å†³ç­–æ ‘è§„åˆ™
   - ç‰¹å¾é‡è¦æ€§æ’å
   - æ¨¡å‹æ€§èƒ½è¯¦æƒ…

4. **æ¶ˆèå®éªŒå›¾** (`results/*_ablation_study_analysis_*.png`)
   - Top-kç‰¹å¾æ•°é‡æ¶ˆèå®éªŒ
   - å†³ç­–æ ‘æ·±åº¦æ¶ˆèå®éªŒ

---

## ï¿½ Experimental Configuration

### å‚æ•°ç©ºé—´
- **Top-kç‰¹å¾æ•°**: 
  - German Dataset: k âˆˆ {5, 6, ..., 54}
  - Australian Dataset: k âˆˆ {5, 6, ..., 22}
  - UCI Dataset: k âˆˆ {5, 6, ..., 23}
- **è’¸é¦æ¸©åº¦**: T âˆˆ {1, 2, 3, 4, 5}
- **æŸå¤±æƒé‡**: Î± âˆˆ {0.0, 0.1, 0.2, ..., 1.0}
- **æ ‘æ·±åº¦**: max_depth âˆˆ {3, 4, 5, 6, 7, 8, 9, 10}

### è¯„ä¼°æŒ‡æ ‡
- **å‡†ç¡®ç‡ (Accuracy)**: æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹
- **F1åˆ†æ•° (F1-Score)**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
- **ç²¾ç¡®ç‡ (Precision)**: æ­£é¢„æµ‹ä¸­çš„æ­£ç¡®æ¯”ä¾‹
- **å¬å›ç‡ (Recall)**: å®é™…æ­£ä¾‹ä¸­çš„é¢„æµ‹æ­£ç¡®æ¯”ä¾‹

### å¹¶å‘ä¼˜åŒ–
- **Windowså¹³å°**: ä½¿ç”¨min(4, cpu_count//2)ä¸ªå¹¶å‘è¿›ç¨‹
- **Linux/Macå¹³å°**: ä½¿ç”¨min(cpu_count-1, cpu_count)ä¸ªå¹¶å‘è¿›ç¨‹
- **è¿›åº¦æ˜¾ç¤º**: é›†æˆtqdmè¿›åº¦æ¡ï¼Œå®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦

---



- **v2.0**: Complete refactoring with SHAP-guided distillation# Run complete analysis pipeline- **Accuracy**: åˆ†ç±»å‡†ç¡®ç‡

- **v1.9**: Enhanced neural network architectures

- **v1.8**: Improved feature name handling and visualizationpython main.py- **Precision**: ç²¾ç¡®ç‡

- **v1.7**: Added comprehensive result management

- **v1.6**: Optimized knowledge distillation pipeline```- **Recall**: å¬å›ç‡



---- **F1-Score**: F1åˆ†æ•°



*This project represents cutting-edge research in explainable AI for financial applications, combining the power of deep learning with the interpretability requirements of financial decision-making.*This will generate three key outputs:- **AUC**: ROCæ›²çº¿ä¸‹é¢ç§¯



1. **Model Comparison Table** (`results/model_comparison_*.xlsx`)---

   - Performance metrics for all four model types

   - Statistical significance tests## ç¯å¢ƒé…ç½®

   - Hyperparameter configurations

### ä¾èµ–å®‰è£…

2. **SHAP Feature Importance Visualization** (`results/shap_feature_importance.png`)```bash

   - Top-8 features for each datasetpip install torch pandas scikit-learn xgboost shap matplotlib openpyxl numpy

   - Comparative importance scores```

   - Cross-dataset feature analysis

### è¿è¡Œç³»ç»Ÿ

3. **Top-k Decision Rules** (`results/best_topk_rules_*.txt`)```bash

   - Interpretable IF-THEN rules from best distilled modelspython main.py

## ğŸ“ˆ Key Findings

### ä¸»è¦å®éªŒç»“æœ
- **Top-kç‰¹å¾è’¸é¦**è¾¾åˆ°ä¸å…¨ç‰¹å¾æ¨¡å‹ç›¸å½“çš„å‡†ç¡®ç‡
- **SHAPå¼•å¯¼çš„ç‰¹å¾é€‰æ‹©**æ˜¾è‘—æå‡äº†æ¨¡å‹å¯è§£é‡Šæ€§  
- **çŸ¥è¯†è’¸é¦**æœ‰æ•ˆç¼©å°äº†å‡†ç¡®ç‡ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„å·®è·
- **æ¸©åº¦ç¼©æ”¾å’ŒæŸå¤±åŠ æƒ**æ˜¯æœ‰æ•ˆè’¸é¦çš„å…³é”®æŠ€æœ¯

### æ€§èƒ½åŸºå‡†æµ‹è¯•

| Dataset | Teacher (DNN) | Baseline Tree | All-Feature Distill | Top-k Distill |
|---------|---------------|---------------|-------------------|---------------|
| German | 0.75-0.78 | 0.70-0.73 | 0.73-0.76 | 0.74-0.77 |
| Australian | 0.85-0.88 | 0.82-0.85 | 0.84-0.87 | 0.85-0.88 |
| UCI Taiwan | 0.80-0.83 | 0.76-0.79 | 0.78-0.81 | 0.79-0.82 |

*æ³¨ï¼šèŒƒå›´åæ˜ ä¸åŒè¶…å‚æ•°é…ç½®ä¸‹çš„æ€§èƒ½å˜åŒ–*

---

## ğŸ“š Technical References

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å‰æ²¿ç ”ç©¶æˆæœï¼š

### çŸ¥è¯†è’¸é¦ç›¸å…³
- **Neural Network Distillation**: Hinton et al. (2015) - æ¸©åº¦ç¼©æ”¾å’Œè½¯æ ‡ç­¾è®­ç»ƒ
- **Tabular Data Distillation**: é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„çŸ¥è¯†è’¸é¦ä¼˜åŒ–

### SHAPå¯è§£é‡ŠAI
- **SHAP Values**: Lundberg & Lee (2017) - TreeExplainerç²¾ç¡®ç‰¹å¾é‡è¦æ€§è®¡ç®—
- **Feature Selection**: åŸºäºSHAPçš„æ™ºèƒ½ç‰¹å¾é€‰æ‹©ç­–ç•¥

### ç¥ç»æ¶æ„è®¾è®¡
- **Residual Networks**: He et al. (2016) - æ®‹å·®è¿æ¥æ”¹å–„æ¢¯åº¦æµ
- **Credit Scoring DNNs**: é’ˆå¯¹ä¿¡ç”¨è¯„åˆ†ä¼˜åŒ–çš„ç¥ç»ç½‘ç»œæ¶æ„

### é‡‘èæœºå™¨å­¦ä¹ 
- **Financial ML**: Lopez de Prado (2018) - é‡‘èé£é™©è¯„ä¼°å’Œå¯è§£é‡Šå»ºæ¨¡
- **Regulatory Compliance**: ç¬¦åˆé‡‘èç›‘ç®¡è¦æ±‚çš„å¯è§£é‡ŠAIæ–¹æ³•

---

## ğŸ”„ Version History

### v2.0.0 (Current) - Enhanced Performance
- âœ… ä¼˜åŒ–æ•™å¸ˆæ¨¡å‹æ¶æ„ï¼Œæå‡Germanæ•°æ®é›†å‡†ç¡®ç‡è‡³75%+
- âœ… å‡å°‘è®­ç»ƒepochsï¼Œæé«˜è®­ç»ƒæ•ˆç‡
- âœ… ç®€åŒ–æ¶ˆèå®éªŒå›¾è¡¨ä¸º1Ã—2å¸ƒå±€
- âœ… æ”¹è¿›SHAPå¯è§†åŒ–é…è‰²æ–¹æ¡ˆ
- âœ… ç¦ç”¨æ–‡ä»¶è‡ªåŠ¨æ¸…ç†åŠŸèƒ½
- âœ… å¢å¼ºWindowså¹³å°å¹¶å‘æ”¯æŒ

### v1.0.0 - Initial Release
- âœ… åŸºç¡€çŸ¥è¯†è’¸é¦æ¡†æ¶
- âœ… SHAPç‰¹å¾é‡è¦æ€§åˆ†æ
- âœ… ä¸‰æ•°æ®é›†æ”¯æŒ
- âœ… åŸºç¡€å¯è§†åŒ–åŠŸèƒ½

---

## ğŸ“§ Contact Information

**Primary Author**: Li Dengjia  
**Email**: lidengjia@hnu.edu.cn  
**Institution**: Hunan University  
**Research Focus**: Financial AI, Knowledge Distillation, Explainable Machine Learning

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Acknowledgments

- UCI Machine Learning Repository for providing the benchmark datasets
- SHAP library developers for interpretability tools
- PyTorch team for the deep learning framework  
- Research community for advances in knowledge distillation and explainable AI

---

## ğŸ“– Citation

If you use this work in your research, please cite:

```bibtex
@misc{li2025shap_distillation,
  title={SHAP-Guided Knowledge Distillation for Credit Scoring},
  author={Li, Dengjia and [Co-authors]},
  year={2025},
  institution={Hunan University},
  note={A comprehensive framework for interpretable credit scoring using SHAP-guided knowledge distillation}
}
```

---

*This project represents ongoing research in interpretable machine learning for financial applications. Contributions and collaborations are welcome.*

**Last Updated**: September 16, 2025  
**Version**: v2.0.0
